{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_page(url):\n",
    "        \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    count = 0\n",
    "    for article in soup.find('ul',{'class':'feed'}).find_all('li',{'class':'row listing '}):\n",
    "        article_ = {}\n",
    "        \n",
    "        title = article.find('a',{'class':'feed-title'}).get_text()\n",
    "        \n",
    "        time = article.find_all('span', {'class':'listing-label'})[1].get_text()\n",
    "        \n",
    "        # url\n",
    "        d_url = article.find('a',{'class':'feed-title'}).get('href')\n",
    "        d_url = 'http://www.hrdive.com' + d_url\n",
    "        \n",
    "        count = count +1\n",
    "        print (count, d_url)\n",
    "\n",
    "        #detail\n",
    "        detail = requests.get(d_url)\n",
    "        detail = BeautifulSoup(detail.content,'html.parser')\n",
    "        \n",
    "        a=' '.join(map(lambda x:x.get_text(),detail.find('div',{'class':'print-wrapper'}).find('div').find_all('li')))\n",
    "        b=' '.join(map(lambda x:x.get_text(),detail.find('div',{'class':'print-wrapper'}).find('div').find_all('p')))\n",
    "        long_text = (a + ' ' + b)\n",
    "                \n",
    "        # remove character\n",
    "        for char in ['\\t','\\n','  ']:\n",
    "            title = title.replace(char,'')\n",
    "            long_text = long_text.replace(char,'')\n",
    "            time = time.replace(char,'')\n",
    "            long_text = long_text.replace(char,'')\n",
    "        \n",
    "        article_['url'] = d_url\n",
    "        article_['title'] = title\n",
    "        article_['tag'] = ''\n",
    "        article_['time'] = time\n",
    "        article_['short_text'] = ''\n",
    "        article_['long_text'] = long_text\n",
    "        \n",
    "        with open('TECH_ANALYTICS_hrdive.csv', 'a', encoding='utf-8') as f:\n",
    "            keys = ['url','title','tag','time','short_text','long_text']\n",
    "            row = ','.join('\"%s\"' % article_[key].replace('\"', '') for key in keys)\n",
    "            f.write(row)\n",
    "            f.write('\\n')\n",
    "    \n",
    "#     next page\n",
    "#     next_page = soup.find(\"a\", {'class': 'control-nav-next'})\n",
    "#     print next_page\n",
    "#     if next_page:\n",
    "#         next_page_link = 'https://www.reuters.com/news/archive/mergersNews' + next_page.get('href')\n",
    "#         result += crawl_page(next_page_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1,51):\n",
    "    search_string = '%20deal'\n",
    "    topic = 'Corporate%20News'\n",
    "    url = 'http://www.hrdive.com/search/?page='+str(x)+'&q=' + search_string + '&selected_facets=&topics=' + topic\n",
    "    print (x)\n",
    "    search_page(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_page(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    count = 0\n",
    "    for article in soup.find('ul',{'class':'feed'}).find_all('li',{'class':'row'}):\n",
    "        article_ = {}\n",
    "        \n",
    "        title = article.find('a',{'class':'feed-title'}).get_text()\n",
    "        short_text = article.find('p',{'class':'feed-description'}).get_text()\n",
    "        time = article.find('span', {'class':'label label-subtle'}).get_text()\n",
    "        \n",
    "        # url\n",
    "        d_url = article.find('a',{'class':'feed-title'}).get('href')\n",
    "        d_url = 'http://www.hrdive.com' + d_url\n",
    "        \n",
    "        count = count +1\n",
    "        print (count, d_url)\n",
    "\n",
    "        #detail\n",
    "        detail = requests.get(d_url)\n",
    "        detail = BeautifulSoup(detail.content,'html.parser')\n",
    "        \n",
    "        a=' '.join(map(lambda x:x.get_text(),detail.find('div',{'class':'print-wrapper'}).find('div').find_all('li')))\n",
    "        b=' '.join(map(lambda x:x.get_text(),detail.find('div',{'class':'print-wrapper'}).find('div').find_all('p')))\n",
    "        long_text = (a + ' ' + b)\n",
    "                \n",
    "        # remove character\n",
    "        for char in ['\\t','\\n','  ']:\n",
    "            title = title.replace(char,'')\n",
    "            long_text = long_text.replace(char,'')\n",
    "            time = time.replace(char,'')\n",
    "            long_text = long_text.replace(char,'')\n",
    "        \n",
    "        article_['url'] = d_url\n",
    "        article_['title'] = title\n",
    "        article_['tag'] = ''\n",
    "        article_['time'] = time\n",
    "        article_['short_text'] = ''\n",
    "        article_['long_text'] = long_text\n",
    "        \n",
    "        with open('COMP_BENEFITS.csv', 'a', encoding='utf-8') as f:\n",
    "            keys = ['url','title','tag','time','short_text','long_text']\n",
    "            row = ','.join('\"%s\"' % article_[key].replace('\"', '') for key in keys)\n",
    "            f.write(row)\n",
    "            f.write('\\n')\n",
    "    \n",
    "#     next page\n",
    "#     next_page = soup.find(\"a\", {'class': 'control-nav-next'})\n",
    "#     print next_page\n",
    "#     if next_page:\n",
    "#         next_page_link = 'https://www.reuters.com/news/archive/mergersNews' + next_page.get('href')\n",
    "#         result += crawl_page(next_page_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1,51):\n",
    "    topic = 'compensation-benefits'\n",
    "    url = 'http://www.hrdive.com/topic/' + topic + '/?page=' + str(x)\n",
    "    print (x)\n",
    "    topic_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
