{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time as t\n",
    "import winsound\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_page(url):\n",
    "    \n",
    "    t.sleep(3)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        t.sleep(5)\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            return\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    for article in soup.find_all('article', {'class':'story'}):\n",
    "        article_ = {}\n",
    "        \n",
    "        title = article.find('h3', {'class':'story-title'}).get_text()\n",
    "        short_text = article.find('p').get_text()\n",
    "        time = article.find('time', {'class':'article-time'}).get_text()        \n",
    "        # url\n",
    "        d_url = article.find('div', {'class':'story-content'}).find('a').get('href')\n",
    "        d_url = 'https://www.reuters.com' + d_url\n",
    "        \n",
    "        print (d_url)\n",
    "\n",
    "        #detail\n",
    "        detail = requests.get(d_url)\n",
    "        if (detail.status_code == 200):\n",
    "            detail = BeautifulSoup(detail.content,'html.parser')\n",
    "        else:\n",
    "            print('sleep')\n",
    "            t.sleep(5)\n",
    "            detail = requests.get(d_url)\n",
    "            detail = BeautifulSoup(detail.content,'html.parser')\n",
    "            if(detail.status_code != 200):\n",
    "                continue\n",
    "        \n",
    "        long_text = detail.find('div',{'class':'ArticleBody_body_2ECha'}).get_text()\n",
    "        \n",
    "        # some little cleanings\n",
    "        for char in ['\\t','\\n','  ']:\n",
    "            title = title.replace(char,'')\n",
    "            short_text = short_text.replace(char,'')\n",
    "            time = time.replace(char,'')\n",
    "            long_text = long_text.replace(char,'')\n",
    "        \n",
    "        article_['url'] = d_url\n",
    "        article_['title'] = title\n",
    "        article_['tag'] = ''\n",
    "        article_['time'] = time\n",
    "        article_['short_text'] = short_text\n",
    "        article_['long_text'] = long_text\n",
    "        \n",
    "        with open('deal_reuter.csv', 'a',encoding = 'utf-8') as f:\n",
    "            keys = ['url','title','tag','time','short_text','long_text']\n",
    "            row = ','.join('\"%s\"' % article_[key].replace('\"', '') for key in keys)\n",
    "            f.write(row)\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic = 'innovationNews'\n",
    "url = 'https://www.reuters.com/news/archive/'+topic+'?view=page&page='\n",
    "for x in range(1,51):\n",
    "    print (x)\n",
    "    topic_page(url + str(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now the search page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_page(url):\n",
    "    t.sleep(3)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        zxc\n",
    "    except:\n",
    "        t.sleep(5)\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except:\n",
    "            return\n",
    "    \n",
    "    # the page return a javascript file contain the data, not json\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    soup = str(soup)\n",
    "    soup = soup.replace('  ','')\n",
    "    soup = soup.replace('\\n','')\n",
    "    soup = soup.replace('\\t','')\n",
    "    soup = soup.replace('\\r','')\n",
    "    soup = soup.partition('news: [')[-1].rpartition(']')[0] # get string between \"news: [\" and \"]\"\n",
    "    soup = soup.split('},')\n",
    "    \n",
    "    count = 1\n",
    "    for s in soup:\n",
    "        try:\n",
    "            article_ = {}\n",
    "            \n",
    "            href = s.partition('href: \"')[-1].rpartition('\",blu')[0] # \"blu\" is after the href attr we want\n",
    "\n",
    "            detail_link = 'https://www.reuters.com' + href\n",
    "\n",
    "            r = requests.get(detail_link)\n",
    "            if (r.status_code==200):\n",
    "                article = BeautifulSoup(r.content, 'html.parser')\n",
    "            else:\n",
    "                print('sleep')\n",
    "                t.sleep(5)\n",
    "                r = requests.get(detail_link)\n",
    "                article = BeautifulSoup(r.content, 'html.parser')\n",
    "                if(r.status_code!=200):\n",
    "                    continue\n",
    "\n",
    "\n",
    "            print (count, detail_link)\n",
    "            count = count + 1\n",
    "\n",
    "            article_['url'] = detail_link\n",
    "            article_['title'] = article.find('h1',{'class':'ArticleHeader_headline_2zdFM'}).get_text()\n",
    "            article_['tag'] = ''\n",
    "            article_['time'] = article.find('div',{'class':'ArticleHeader_date_V9eGk'}).get_text()\n",
    "            article_['short_text'] = ''\n",
    "            article_['long_text'] = article.find('div',{'class':'ArticleBody_body_2ECha'}).get_text()\n",
    "\n",
    "            keys = ['url','title','tag','time','short_text','long_text']\n",
    "\n",
    "            with open('new_ceo_reuter.csv', 'a',encoding = 'utf8') as f:\n",
    "                keys = ['url','title','tag','time','short_text','long_text']\n",
    "                row = ','.join('\"%s\"' % article_[key].replace('\"', '') for key in keys)\n",
    "                f.write(row)\n",
    "                f.write('\\n')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1,2):\n",
    "    search_string = 'new+ceo'\n",
    "    url = 'https://www.reuters.com/assets/searchArticleLoadMoreJson?blob='+search_string+'&bigOrSmall=big&articleWithBlog=true&sortBy=&dateRange=&numResultsToShow=10&pn='+str(x)+'&callback=addMoreNewsResults'\n",
    "    print (x)\n",
    "    search_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
